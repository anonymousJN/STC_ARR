{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers_local import models, losses, SentenceTransformerSequential\n",
    "from models.Transformers import SCCLBert\n",
    "from learners.cluster import ClusterLearner\n",
    "from dataloader.dataloader import augment_loader, augment_loader_split\n",
    "\n",
    "from training import training\n",
    "#from training_error_analysis import training\n",
    "\n",
    "from utils.kmeans import get_kmeans_centers\n",
    "from utils.logger import setup_path\n",
    "from utils.randomness import set_global_random_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n",
      "Tesla V100-SXM2-32GB-LS\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2'\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = {\n",
    "    \"distil\": 'distilbert-base-nli-stsb-mean-tokens', \n",
    "    \"robertabase\": 'roberta-base-nli-stsb-mean-tokens',\n",
    "    \"robertalarge\": 'roberta-large-nli-stsb-mean-tokens',\n",
    "    \"msmarco\": 'distilroberta-base-msmarco-v2',\n",
    "    \"xlm\": \"xlm-r-distilroberta-base-paraphrase-v1\",\n",
    "    \"bertlarge\": 'bert-large-nli-stsb-mean-tokens',\n",
    "    \"bertbase\": 'bert-base-nli-stsb-mean-tokens',\n",
    "    \"paraphrase\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-distil\": \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-Tiny\" : \"paraphrase-TinyBERT-L6-v2\"\n",
    "}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0], help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "parser.add_argument('--seed', type=int, default=0, help=\"\")\n",
    "parser.add_argument('--print_freq', type=float, default=100, help=\"\")  \n",
    "parser.add_argument('--result_path', type=str, default='./results/')\n",
    "\n",
    "parser.add_argument('--bert', type=str, default='paraphrase', help=\"\")\n",
    "#parser.add_argument('--bert', type=str, default='distil', help=\"\")\n",
    "\n",
    "parser.add_argument('--bert_model', type=str, default='bert-base-uncased', help=\"\")\n",
    "parser.add_argument('--note', type=str, default='_search_snippets_distil_lre-4_JSD', help=\"\")\n",
    "\n",
    "# Dataset\n",
    "# stackoverflow/stackoverflow_true_text\n",
    "parser.add_argument('--dataset', type=str, default='search_snippets', help=\"\")\n",
    "#parser.add_argument('--dataset', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "parser.add_argument('--max_length', type=int, default=32)\n",
    "parser.add_argument('--train_val_ratio', type=float, default= [0.9, 0.1])\n",
    "\n",
    "# Data for train and test\n",
    "# ###### AgNews\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "parser.add_argument('--dataname', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "parser.add_argument('--dataname_val', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "parser.add_argument('--num_classes', type=int, default=4, help=\"\")\n",
    "# ####### SearchSnippets\n",
    "parser.add_argument('--data_path', type=str, default='./datasets/augmented/contextual_20_2col_bert/')\n",
    "# ## parser.add_argument('--dataname', type=str, default='train_search_snippets.csv', help=\"\")\n",
    "## parser.add_argument('--dataname_val', type=str, default='test_search_snippets.csv', help=\"\")\n",
    "# parser.add_argument('--dataname', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=8, help=\"\")\n",
    "# # ###### StackOverFlow\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "# parser.add_argument('--dataname', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='stackoverflow_', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ###### Biomedical\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/biomedical/')\n",
    "# parser.add_argument('--dataname', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ######## Tweet\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=89, help=\"\")\n",
    "# ######## GoogleNewsTS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsT\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "\n",
    "# Learning parameters\n",
    "parser.add_argument('--lr', type=float, default=1e-6, help=\"\") #learning rate\n",
    "parser.add_argument('--lr_scale', type=int, default=100, help=\"\")\n",
    "parser.add_argument('--max_iter', type=int, default=30000)\n",
    "parser.add_argument('--batch_size', type=int, default=256) #batch size\n",
    "\n",
    "# CNN Setting\n",
    "#parser.add_argument('--out_channels', type=int, default=768)\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_1')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_3')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_5')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_7')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_cat')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_avg')\n",
    "\n",
    "# Contrastive learning\n",
    "parser.add_argument('--use_head', type=bool, default=False)\n",
    "parser.add_argument('--use_normalize', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--weighted_local', type=bool, default=False, help=\"\")\n",
    "#parser.add_argument('--normalize_method', type=str, default='inverse_prob', help=\"\")\n",
    "parser.add_argument('--normalize_method', type=str, default='none', help=\"\")\n",
    "\n",
    "parser.add_argument('--contrastive_local_scale', type=float, default=0.002) #scale of contrastive loss\n",
    "parser.add_argument('--contrastive_global_scale', type=float, default=0.008) #scale of contrastive loss\n",
    "parser.add_argument('--temperature', type=float, default=0.5, help=\"temperature required by contrastive loss\")\n",
    "parser.add_argument('--base_temperature', type=float, default=0.1, help=\"temperature required by contrastive loss\")\n",
    "\n",
    "# Clustering\n",
    "#parser.add_argument('--clustering_scale', type=float, default=0.02) #scale of clustering loss\n",
    "parser.add_argument('--clustering_scale', type=float, default=0.02)\n",
    "parser.add_argument('--use_perturbation', action='store_true', help=\"\")\n",
    "parser.add_argument('--alpha', type=float, default=1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args.use_gpu = args.gpuid[0] >= 0\n",
    "args.resPath = None\n",
    "args.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results path: ./results/SCCL.paraphrase.search_snippets.lr1e-06.lrscale100.tmp0.5.alpha1.seed0/\n",
      "all_embeddings:(8000, 768), true_labels:8000, pred_labels:8000\n",
      "true_labels tensor([1, 1, 3,  ..., 3, 3, 0])\n",
      "pred_labels tensor([0, 0, 2,  ..., 2, 1, 1], dtype=torch.int32)\n",
      "Iterations:12, Clustering ACC:0.835, centers:(4, 768)\n",
      "initial_cluster_centers =  torch.Size([4, 768])\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 6e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5.9999999999999995e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "train_sample 0.9 7200\n",
      "val_sample 0.1 800\n",
      "\n",
      "=30000/29=Iterations/Batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_length_0_1 mean: 26.3359375\n",
      "[0]-----\n",
      "contrastive_local_loss:\t 0.01226\n",
      "contrastive_global_loss:\t 0.00971\n",
      "clustering_loss:\t 0.00007\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 29 batches -------------\n",
      "all_pred 4\n",
      "[Representation] Clustering scores: {'NMI': 0.5788365167426238, 'ARI': 0.6206423535670139, 'AMI': 0.5786459523427855}\n",
      "[Representation] ACC: 0.8351\n",
      "[Representation] ACC sklearn: 0.0436\n",
      "[Model] Clustering scores: {'NMI': 0.5778521904796246, 'ARI': 0.6170467776857887, 'AMI': 0.577661145322776}\n",
      "[Model] ACC: 0.8325\n",
      "[Model] ACC sklearn: 0.0708\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 4 batches -------------\n",
      "all_pred 4\n",
      "[Representation] Clustering scores: {'NMI': 0.5750889693915782, 'ARI': 0.601199355613092, 'AMI': 0.5733375323972522}\n",
      "[Representation] ACC: 0.8250\n",
      "[Representation] ACC sklearn: 0.4100\n",
      "[Model] Clustering scores: {'NMI': 0.6003168949110672, 'ARI': 0.638049549342291, 'AMI': 0.5986724925045189}\n",
      "[Model] ACC: 0.8438\n",
      "[Model] ACC sklearn: 0.0638\n",
      "sentence_length_0_1 mean: 26.2734375\n",
      "sentence_length_0_1 mean: 26.7734375\n",
      "sentence_length_0_1 mean: 26.02734375\n",
      "sentence_length_0_1 mean: 26.443359375\n",
      "sentence_length_0_1 mean: 26.638671875\n",
      "sentence_length_0_1 mean: 25.908203125\n",
      "sentence_length_0_1 mean: 26.8671875\n",
      "sentence_length_0_1 mean: 26.05859375\n",
      "sentence_length_0_1 mean: 27.025390625\n",
      "sentence_length_0_1 mean: 26.46875\n",
      "sentence_length_0_1 mean: 26.10546875\n",
      "sentence_length_0_1 mean: 26.544921875\n",
      "sentence_length_0_1 mean: 26.2421875\n",
      "sentence_length_0_1 mean: 26.123046875\n",
      "sentence_length_0_1 mean: 25.31640625\n",
      "sentence_length_0_1 mean: 26.376953125\n",
      "sentence_length_0_1 mean: 25.92578125\n",
      "sentence_length_0_1 mean: 25.634765625\n",
      "sentence_length_0_1 mean: 26.412109375\n",
      "sentence_length_0_1 mean: 26.0234375\n",
      "sentence_length_0_1 mean: 26.759765625\n",
      "sentence_length_0_1 mean: 26.3828125\n",
      "sentence_length_0_1 mean: 26.564453125\n",
      "sentence_length_0_1 mean: 26.861328125\n",
      "sentence_length_0_1 mean: 25.724609375\n",
      "sentence_length_0_1 mean: 26.6171875\n",
      "sentence_length_0_1 mean: 26.189453125\n",
      "sentence_length_0_1 mean: 26.453125\n",
      "sentence_length_0_1 mean: 26.505859375\n",
      "sentence_length_0_1 mean: 25.630859375\n",
      "sentence_length_0_1 mean: 26.431640625\n",
      "sentence_length_0_1 mean: 26.599609375\n",
      "sentence_length_0_1 mean: 26.40234375\n",
      "sentence_length_0_1 mean: 25.86328125\n",
      "sentence_length_0_1 mean: 25.97265625\n",
      "sentence_length_0_1 mean: 26.056640625\n",
      "sentence_length_0_1 mean: 26.419921875\n",
      "sentence_length_0_1 mean: 26.623046875\n",
      "sentence_length_0_1 mean: 26.095703125\n",
      "sentence_length_0_1 mean: 25.375\n",
      "sentence_length_0_1 mean: 26.2734375\n",
      "sentence_length_0_1 mean: 25.9765625\n",
      "sentence_length_0_1 mean: 26.41796875\n",
      "sentence_length_0_1 mean: 26.265625\n",
      "sentence_length_0_1 mean: 26.255859375\n",
      "sentence_length_0_1 mean: 27.08203125\n",
      "sentence_length_0_1 mean: 26.541015625\n",
      "sentence_length_0_1 mean: 26.830078125\n",
      "sentence_length_0_1 mean: 26.236328125\n",
      "sentence_length_0_1 mean: 26.609375\n",
      "sentence_length_0_1 mean: 26.5390625\n",
      "sentence_length_0_1 mean: 26.376953125\n",
      "sentence_length_0_1 mean: 26.46484375\n",
      "sentence_length_0_1 mean: 26.900390625\n",
      "sentence_length_0_1 mean: 26.166015625\n",
      "sentence_length_0_1 mean: 25.998046875\n",
      "sentence_length_0_1 mean: 24.171875\n",
      "sentence_length_0_1 mean: 25.8671875\n",
      "sentence_length_0_1 mean: 26.517578125\n",
      "sentence_length_0_1 mean: 26.474609375\n",
      "sentence_length_0_1 mean: 26.6875\n",
      "sentence_length_0_1 mean: 26.263671875\n",
      "sentence_length_0_1 mean: 26.447265625\n",
      "sentence_length_0_1 mean: 25.76953125\n",
      "sentence_length_0_1 mean: 26.51171875\n",
      "sentence_length_0_1 mean: 25.95703125\n",
      "sentence_length_0_1 mean: 26.1875\n",
      "sentence_length_0_1 mean: 25.603515625\n",
      "sentence_length_0_1 mean: 26.173828125\n",
      "sentence_length_0_1 mean: 26.869140625\n",
      "sentence_length_0_1 mean: 26.1796875\n",
      "sentence_length_0_1 mean: 26.3984375\n",
      "sentence_length_0_1 mean: 26.5\n",
      "sentence_length_0_1 mean: 26.357421875\n",
      "sentence_length_0_1 mean: 26.5\n",
      "sentence_length_0_1 mean: 25.9375\n",
      "sentence_length_0_1 mean: 26.326171875\n",
      "sentence_length_0_1 mean: 26.06640625\n",
      "sentence_length_0_1 mean: 26.2734375\n",
      "sentence_length_0_1 mean: 26.509765625\n",
      "sentence_length_0_1 mean: 26.318359375\n",
      "sentence_length_0_1 mean: 26.45703125\n",
      "sentence_length_0_1 mean: 26.755859375\n",
      "sentence_length_0_1 mean: 26.84375\n",
      "sentence_length_0_1 mean: 25.9140625\n",
      "sentence_length_0_1 mean: 26.109375\n",
      "sentence_length_0_1 mean: 26.5\n",
      "sentence_length_0_1 mean: 26.73828125\n",
      "sentence_length_0_1 mean: 26.544921875\n",
      "sentence_length_0_1 mean: 26.31640625\n",
      "sentence_length_0_1 mean: 26.650390625\n",
      "sentence_length_0_1 mean: 26.908203125\n",
      "sentence_length_0_1 mean: 26.009765625\n",
      "sentence_length_0_1 mean: 26.5\n",
      "sentence_length_0_1 mean: 26.20703125\n",
      "sentence_length_0_1 mean: 25.541015625\n",
      "sentence_length_0_1 mean: 26.197265625\n",
      "sentence_length_0_1 mean: 25.841796875\n",
      "sentence_length_0_1 mean: 25.58203125\n",
      "sentence_length_0_1 mean: 26.138671875\n",
      "[100]-----\n",
      "contrastive_local_loss:\t 0.00747\n",
      "contrastive_global_loss:\t 0.00591\n",
      "clustering_loss:\t 0.00005\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 29 batches -------------\n"
     ]
    }
   ],
   "source": [
    "resPath, tensorboard = setup_path(args)\n",
    "args.resPath, args.tensorboard = resPath, tensorboard\n",
    "set_global_random_seed(args.seed)\n",
    "\n",
    "# Dataset loader\n",
    "train_loader = augment_loader(args)\n",
    "\n",
    "# torch.cuda.set_device(args.gpuid[0])\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# Initialize cluster centers\n",
    "# by performing k-means after getting embeddings from Sentence-BERT with mean-pooling(defualt)\n",
    "sbert = SentenceTransformer(MODEL_CLASS[args.bert])\n",
    "cluster_centers = get_kmeans_centers(sbert, train_loader, args.num_classes) \n",
    "\n",
    "\n",
    "# Model\n",
    "# 1. Transformer model \n",
    "# use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "# word_embedding_model = models.Transformer(MODEL_CLASS[args.bert])\n",
    "\n",
    "word_embedding_model = models.Transformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "#word_embedding_model = models.Transformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "dimension = word_embedding_model.get_word_embedding_dimension()\n",
    "# word_embedding_model = torch.nn.DataParallel(word_embedding_model)\n",
    "\n",
    "\n",
    "# 2. CNN model\n",
    "# cnn = models.CNN(in_word_embedding_dimension = word_embedding_model.get_word_embedding_dimension(), \n",
    "#                  use_cnn = args.use_cnn, out_channels = word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. Pooling \n",
    "# pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
    "#                                pooling_mode_mean_tokens=True,\n",
    "#                                pooling_mode_cls_token=False,\n",
    "#                                pooling_mode_max_tokens=False)\n",
    "pooling_model = models.Pooling(dimension,\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False, \n",
    "                               pooling_mode_weighted_tokens=False)\n",
    "\n",
    "# 4. Feature extractor \n",
    "#feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, cnn, pooling_model])\n",
    "feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, pooling_model], device = 'cuda')\n",
    "\n",
    "# 5. main model\n",
    "model = SCCLBert(feature_extractor, cluster_centers=cluster_centers, alpha = args.alpha, use_head = args.use_head)  \n",
    "\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':word_embedding_model.parameters(), 'lr': args.lr*6},\n",
    "#    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "    {'params':pooling_model.parameters()},\n",
    "#    {'params':model.head.parameters(), 'lr': args.lr*20},\n",
    "    {'params':model.cluster_centers, 'lr': args.lr*60}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params':word_embedding_model.parameters(), 'lr': args.lr},\n",
    "# #    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "#     {'params':pooling_model.parameters()},\n",
    "# #    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "#     {'params':model.cluster_centers, 'lr': args.lr*20}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "# Set up the trainer    \n",
    "learner = ClusterLearner(model, feature_extractor, optimizer, args.temperature, args.base_temperature,\n",
    "                         args.contrastive_local_scale, args.contrastive_global_scale, args.clustering_scale, use_head = args.use_head, use_normalize = args.use_normalize)\n",
    "# learner = torch.nn.DataParallel(learner)\n",
    "learner = learner.cuda()\n",
    "\n",
    "# split train - validation\n",
    "if(args.train_val_ratio != -1):\n",
    "    train_loader, val_loader = augment_loader_split(args)\n",
    "    training(train_loader, learner, args, val_loader = val_loader)\n",
    "# normal\n",
    "else:\n",
    "    training(train_loader, learner, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
