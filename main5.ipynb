{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers_local import models, losses, SentenceTransformerSequential\n",
    "from models.Transformers import SCCLBert\n",
    "from learners.cluster import ClusterLearner\n",
    "from dataloader.dataloader import augment_loader, augment_loader_split\n",
    "\n",
    "from training import training\n",
    "#from training_error_analysis import training\n",
    "\n",
    "from utils.kmeans import get_kmeans_centers\n",
    "from utils.logger import setup_path\n",
    "from utils.randomness import set_global_random_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n",
      "Tesla V100-SXM2-32GB-LS\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='7'\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = {\n",
    "    \"distil\": 'distilbert-base-nli-stsb-mean-tokens', \n",
    "    \"robertabase\": 'roberta-base-nli-stsb-mean-tokens',\n",
    "    \"robertalarge\": 'roberta-large-nli-stsb-mean-tokens',\n",
    "    \"msmarco\": 'distilroberta-base-msmarco-v2',\n",
    "    \"xlm\": \"xlm-r-distilroberta-base-paraphrase-v1\",\n",
    "    \"bertlarge\": 'bert-large-nli-stsb-mean-tokens',\n",
    "    \"bertbase\": 'bert-base-nli-stsb-mean-tokens',\n",
    "    \"paraphrase\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-distil\": \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-Tiny\" : \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"stanford-sentiment-roberta\" : \"stanford-sentiment-treebank-roberta.2021-03-11\"\n",
    "}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0], help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "parser.add_argument('--seed', type=int, default=0, help=\"\")\n",
    "parser.add_argument('--print_freq', type=float, default=200, help=\"\")  \n",
    "parser.add_argument('--result_path', type=str, default='./results/')\n",
    "\n",
    "parser.add_argument('--bert', type=str, default='paraphrase', help=\"\")\n",
    "#parser.add_argument('--bert', type=str, default='distil', help=\"\")\n",
    "\n",
    "parser.add_argument('--bert_model', type=str, default='bert-base-uncased', help=\"\")\n",
    "parser.add_argument('--note', type=str, default='_search_snippets_distil_lre-4_JSD', help=\"\")\n",
    "\n",
    "# Dataset\n",
    "# stackoverflow/stackoverflow_true_text\n",
    "parser.add_argument('--dataset', type=str, default='search_snippets', help=\"\")\n",
    "#parser.add_argument('--dataset', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "parser.add_argument('--max_length', type=int, default=32)\n",
    "parser.add_argument('--train_val_ratio', type=float, default= [0.9, 0.1])\n",
    "\n",
    "# Data for train and test\n",
    "# ###### AgNews\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=4, help=\"\")\n",
    "# ####### SearchSnippets\n",
    "parser.add_argument('--data_path', type=str, default='./datasets/augmented/contextual_30_2col_distilbert/')\n",
    "# ## parser.add_argument('--dataname', type=str, default='train_search_snippets.csv', help=\"\")\n",
    "# ## parser.add_argument('--dataname_val', type=str, default='test_search_snippets.csv', help=\"\")\n",
    "# parser.add_argument('--dataname', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=8, help=\"\")\n",
    "# ###### StackOverFlow\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "# parser.add_argument('--dataname', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ###### Biomedical\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/biomedical/')\n",
    "# parser.add_argument('--dataname', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ######## Tweet\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=89, help=\"\")\n",
    "# ######## GoogleNewsTS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "parser.add_argument('--dataname', type=str, default='TS', help=\"\")\n",
    "parser.add_argument('--dataname_val', type=str, default='TS', help=\"\")\n",
    "parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsT\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "\n",
    "# Learning parameters\n",
    "parser.add_argument('--lr', type=float, default=1e-6, help=\"\") #learning rate\n",
    "parser.add_argument('--lr_scale', type=int, default=100, help=\"\")\n",
    "parser.add_argument('--max_iter', type=int, default=30000)\n",
    "parser.add_argument('--batch_size', type=int, default=256) #batch size\n",
    "\n",
    "# CNN Setting\n",
    "#parser.add_argument('--out_channels', type=int, default=768)\n",
    "#parser.add_argument('--use_cnn', type5yh=str, default='cnn_1')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_3')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_5')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_7')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_cat')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_avg')\n",
    "\n",
    "# Contrastive learning\n",
    "parser.add_argument('--use_head', type=bool, default=False)\n",
    "parser.add_argument('--use_normalize', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--weighted_local', type=bool, default=False, help=\"\")\n",
    "#parser.add_argument('--normalize_method', type=str, default='inverse_prob', help=\"\")\n",
    "parser.add_argument('--normalize_method', type=str, default='none', help=\"\")\n",
    "\n",
    "parser.add_argument('--contrastive_local_scale', type=float, default=0.002) #scale of contrastive loss\n",
    "parser.add_argument('--contrastive_global_scale', type=float, default=0.008) #scale of contrastive loss\n",
    "parser.add_argument('--temperature', type=float, default=0.5, help=\"temperature required by contrastive loss\")\n",
    "parser.add_argument('--base_temperature', type=float, default=0.1, help=\"temperature required by contrastive loss\")\n",
    "\n",
    "# Clustering\n",
    "# default = 0.02\n",
    "parser.add_argument('--clustering_scale', type=float, default=0.02) #scale of clustering loss\n",
    "parser.add_argument('--use_perturbation', action='store_true', help=\"\")\n",
    "parser.add_argument('--alpha', type=float, default=1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args.use_gpu = args.gpuid[0] >= 0\n",
    "args.resPath = None\n",
    "args.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results path: ./results/SCCL.paraphrase.search_snippets.lr1e-06.lrscale100.tmp0.5.alpha1.seed0/\n",
      "all_embeddings:(11109, 768), true_labels:11109, pred_labels:11109\n",
      "true_labels tensor([141,  38, 134,  ...,  57,  24,  17])\n",
      "pred_labels tensor([138,  55, 136,  ..., 115, 113, 134], dtype=torch.int32)\n",
      "Iterations:17, Clustering ACC:0.722, centers:(152, 768)\n",
      "initial_cluster_centers =  torch.Size([152, 768])\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 6e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5.9999999999999995e-05\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample 0.9 9999\n",
      "val_sample 0.1 1110\n",
      "\n",
      "=30000/40=Iterations/Batches\n",
      "[0]-----\n",
      "contrastive_local_loss:\t 0.01404\n",
      "contrastive_global_loss:\t 0.01074\n",
      "clustering_loss:\t 0.00058\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.9089695510749174, 'ARI': 0.6683150882652075, 'AMI': 0.8896471065054697}\n",
      "[Representation] ACC: 0.7158\n",
      "[Representation] ACC sklearn: 0.0201\n",
      "[Model] Clustering scores: {'NMI': 0.9125219645730526, 'ARI': 0.6809755101101889, 'AMI': 0.8937185269253776}\n",
      "[Model] ACC: 0.7187\n",
      "[Model] ACC sklearn: 0.0115\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 151\n",
      "[Representation] Clustering scores: {'NMI': 0.9016110826000304, 'ARI': 0.5875531782979277, 'AMI': 0.7962051819950219}\n",
      "[Representation] ACC: 0.6604\n",
      "[Representation] ACC sklearn: 0.0054\n",
      "[Model] Clustering scores: {'NMI': 0.9237790467110212, 'ARI': 0.664195565552717, 'AMI': 0.8448015030428364}\n",
      "[Model] ACC: 0.7279\n",
      "[Model] ACC sklearn: 0.0117\n",
      "[200]-----\n",
      "contrastive_local_loss:\t 0.00789\n",
      "contrastive_global_loss:\t 0.00589\n",
      "clustering_loss:\t 0.00051\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.9131848568335738, 'ARI': 0.6964048669550321, 'AMI': 0.8948525054056448}\n",
      "[Representation] ACC: 0.7403\n",
      "[Representation] ACC sklearn: 0.0049\n",
      "[Model] Clustering scores: {'NMI': 0.9140121222816597, 'ARI': 0.6855062988528929, 'AMI': 0.8955475767509927}\n",
      "[Model] ACC: 0.7262\n",
      "[Model] ACC sklearn: 0.0102\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 151\n",
      "[Representation] Clustering scores: {'NMI': 0.9084922195889048, 'ARI': 0.6160083633977014, 'AMI': 0.8118140689707032}\n",
      "[Representation] ACC: 0.6973\n",
      "[Representation] ACC sklearn: 0.0018\n",
      "[Model] Clustering scores: {'NMI': 0.922653419694162, 'ARI': 0.6604183400132279, 'AMI': 0.8427055974780635}\n",
      "[Model] ACC: 0.7288\n",
      "[Model] ACC sklearn: 0.0108\n",
      "[400]-----\n",
      "contrastive_local_loss:\t 0.00836\n",
      "contrastive_global_loss:\t 0.00591\n",
      "clustering_loss:\t 0.00095\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.9164144255606554, 'ARI': 0.6908017359354641, 'AMI': 0.8986938237509708}\n",
      "[Representation] ACC: 0.7304\n",
      "[Representation] ACC sklearn: 0.0150\n",
      "[Model] Clustering scores: {'NMI': 0.919273706539919, 'ARI': 0.7023189153500369, 'AMI': 0.9020037675915459}\n",
      "[Model] ACC: 0.7367\n",
      "[Model] ACC sklearn: 0.0084\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 151\n",
      "[Representation] Clustering scores: {'NMI': 0.9220837915195607, 'ARI': 0.6573894672979094, 'AMI': 0.8415450125096418}\n",
      "[Representation] ACC: 0.7333\n",
      "[Representation] ACC sklearn: 0.0126\n",
      "[Model] Clustering scores: {'NMI': 0.9269053257098118, 'ARI': 0.6819723720534454, 'AMI': 0.8518227895475247}\n",
      "[Model] ACC: 0.7495\n",
      "[Model] ACC sklearn: 0.0099\n",
      "[600]-----\n",
      "contrastive_local_loss:\t 0.00966\n",
      "contrastive_global_loss:\t 0.00602\n",
      "clustering_loss:\t 0.00210\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.9245302928813816, 'ARI': 0.7493833606000989, 'AMI': 0.9088217881555362}\n",
      "[Representation] ACC: 0.7859\n",
      "[Representation] ACC sklearn: 0.0187\n",
      "[Model] Clustering scores: {'NMI': 0.9202287975087539, 'ARI': 0.7098932476924724, 'AMI': 0.9032894298268427}\n",
      "[Model] ACC: 0.7443\n",
      "[Model] ACC sklearn: 0.0064\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 150\n",
      "[Representation] Clustering scores: {'NMI': 0.9327836989375468, 'ARI': 0.7541692460984475, 'AMI': 0.864844819920924}\n",
      "[Representation] ACC: 0.7775\n",
      "[Representation] ACC sklearn: 0.0009\n",
      "[Model] Clustering scores: {'NMI': 0.929479127510448, 'ARI': 0.690168652456079, 'AMI': 0.8575590779182468}\n",
      "[Model] ACC: 0.7514\n",
      "[Model] ACC sklearn: 0.0063\n",
      "[800]-----\n",
      "contrastive_local_loss:\t 0.01160\n",
      "contrastive_global_loss:\t 0.00610\n",
      "clustering_loss:\t 0.00396\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 150\n",
      "[Representation] Clustering scores: {'NMI': 0.937665056791925, 'ARI': 0.8272666442717097, 'AMI': 0.9249957243167845}\n",
      "[Representation] ACC: 0.8402\n",
      "[Representation] ACC sklearn: 0.0109\n",
      "[Model] Clustering scores: {'NMI': 0.9180817372035972, 'ARI': 0.7177963497278873, 'AMI': 0.9011509942973717}\n",
      "[Model] ACC: 0.7548\n",
      "[Model] ACC sklearn: 0.0043\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 147\n",
      "[Representation] Clustering scores: {'NMI': 0.954432532479609, 'ARI': 0.8635687764782353, 'AMI': 0.910914299648541}\n",
      "[Representation] ACC: 0.8523\n",
      "[Representation] ACC sklearn: 0.0063\n",
      "[Model] Clustering scores: {'NMI': 0.9311369448970799, 'ARI': 0.7057367689091869, 'AMI': 0.8631338332948143}\n",
      "[Model] ACC: 0.7640\n",
      "[Model] ACC sklearn: 0.0045\n",
      "[Representation] Clustering scores: {'NMI': 0.9422447906102016, 'ARI': 0.8419242561170084, 'AMI': 0.9308643926984833}\n",
      "[Representation] ACC: 0.8505\n",
      "[Representation] ACC sklearn: 0.0129\n",
      "[Model] Clustering scores: {'NMI': 0.9158747094781854, 'ARI': 0.7294018838179822, 'AMI': 0.8993266427163733}\n",
      "[Model] ACC: 0.7720\n",
      "[Model] ACC sklearn: 0.0021\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 140\n",
      "[Representation] Clustering scores: {'NMI': 0.9532317708164745, 'ARI': 0.847505847766201, 'AMI': 0.9104763786465171}\n",
      "[Representation] ACC: 0.8541\n",
      "[Representation] ACC sklearn: 0.0477\n",
      "[Model] Clustering scores: {'NMI': 0.9319421488624289, 'ARI': 0.730977677994693, 'AMI': 0.8690165547916916}\n",
      "[Model] ACC: 0.7919\n",
      "[Model] ACC sklearn: 0.0027\n",
      "[1200]-----\n",
      "contrastive_local_loss:\t 0.01742\n",
      "contrastive_global_loss:\t 0.00665\n",
      "clustering_loss:\t 0.00909\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 144\n",
      "[Representation] Clustering scores: {'NMI': 0.9462178896516741, 'ARI': 0.8479233481553049, 'AMI': 0.9359214823301413}\n",
      "[Representation] ACC: 0.8614\n",
      "[Representation] ACC sklearn: 0.0278\n",
      "[Model] Clustering scores: {'NMI': 0.9158764565928939, 'ARI': 0.755073350610682, 'AMI': 0.8999039546045364}\n",
      "[Model] ACC: 0.7790\n",
      "[Model] ACC sklearn: 0.0015\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 134\n",
      "[Representation] Clustering scores: {'NMI': 0.9601900965258039, 'ARI': 0.869118205536366, 'AMI': 0.9251903679310336}\n",
      "[Representation] ACC: 0.8766\n",
      "[Representation] ACC sklearn: 0.0027\n",
      "[Model] Clustering scores: {'NMI': 0.9340140874118256, 'ARI': 0.7985733812279099, 'AMI': 0.8759857347021898}\n",
      "[Model] ACC: 0.8108\n",
      "[Model] ACC sklearn: 0.0018\n",
      "[1400]-----\n",
      "contrastive_local_loss:\t 0.01972\n",
      "contrastive_global_loss:\t 0.00663\n",
      "clustering_loss:\t 0.01142\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 140\n",
      "[Representation] Clustering scores: {'NMI': 0.9464111736977311, 'ARI': 0.8529607627359097, 'AMI': 0.9362034976544162}\n",
      "[Representation] ACC: 0.8625\n",
      "[Representation] ACC sklearn: 0.0166\n",
      "[Model] Clustering scores: {'NMI': 0.9070724819017, 'ARI': 0.754817291385228, 'AMI': 0.8902916623553504}\n",
      "[Model] ACC: 0.7691\n",
      "[Model] ACC sklearn: 0.0013\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 123\n",
      "[Representation] Clustering scores: {'NMI': 0.9605241776700252, 'ARI': 0.8644952395618383, 'AMI': 0.9264125233416156}\n",
      "[Representation] ACC: 0.8820\n",
      "[Representation] ACC sklearn: 0.0000\n",
      "[Model] Clustering scores: {'NMI': 0.9242854942438713, 'ARI': 0.7905417287203712, 'AMI': 0.8607782054086888}\n",
      "[Model] ACC: 0.7937\n",
      "[Model] ACC sklearn: 0.0018\n",
      "[1600]-----\n",
      "contrastive_local_loss:\t 0.02076\n",
      "contrastive_global_loss:\t 0.00663\n",
      "clustering_loss:\t 0.01247\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_pred 131\n",
      "[Representation] Clustering scores: {'NMI': 0.9389832839410392, 'ARI': 0.8365898737750431, 'AMI': 0.9276295675852931}\n",
      "[Representation] ACC: 0.8468\n",
      "[Representation] ACC sklearn: 0.0066\n",
      "[Model] Clustering scores: {'NMI': 0.8985647127003126, 'ARI': 0.7515461263651261, 'AMI': 0.8810671713043842}\n",
      "[Model] ACC: 0.7528\n",
      "[Model] ACC sklearn: 0.0014\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 119\n",
      "[Representation] Clustering scores: {'NMI': 0.9576414152895678, 'ARI': 0.8654929747709065, 'AMI': 0.9212546020413577}\n",
      "[Representation] ACC: 0.8721\n",
      "[Representation] ACC sklearn: 0.0189\n",
      "[Model] Clustering scores: {'NMI': 0.9233822509425157, 'ARI': 0.7920474378999568, 'AMI': 0.8606109001818163}\n",
      "[Model] ACC: 0.7838\n",
      "[Model] ACC sklearn: 0.0018\n",
      "[1800]-----\n",
      "contrastive_local_loss:\t 0.02205\n",
      "contrastive_global_loss:\t 0.00664\n",
      "clustering_loss:\t 0.01373\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 128\n",
      "[Representation] Clustering scores: {'NMI': 0.9319950820804246, 'ARI': 0.8254809003962025, 'AMI': 0.9195112130900388}\n",
      "[Representation] ACC: 0.8339\n",
      "[Representation] ACC sklearn: 0.0003\n",
      "[Model] Clustering scores: {'NMI': 0.8929396678726695, 'ARI': 0.7578036733885971, 'AMI': 0.8754996306280092}\n",
      "[Model] ACC: 0.7577\n",
      "[Model] ACC sklearn: 0.0015\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 112\n",
      "[Representation] Clustering scores: {'NMI': 0.9534760858366153, 'ARI': 0.8577062009785095, 'AMI': 0.9140050914914866}\n",
      "[Representation] ACC: 0.8640\n",
      "[Representation] ACC sklearn: 0.0297\n",
      "[Model] Clustering scores: {'NMI': 0.9203577154176209, 'ARI': 0.7901137928912492, 'AMI': 0.8574214361226592}\n",
      "[Model] ACC: 0.7766\n",
      "[Model] ACC sklearn: 0.0018\n",
      "[2000]-----\n",
      "contrastive_local_loss:\t 0.02231\n",
      "contrastive_global_loss:\t 0.00678\n",
      "clustering_loss:\t 0.01382\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 121\n",
      "[Representation] Clustering scores: {'NMI': 0.9324945918839427, 'ARI': 0.82458107602783, 'AMI': 0.9201555537881482}\n",
      "[Representation] ACC: 0.8375\n",
      "[Representation] ACC sklearn: 0.0268\n",
      "[Model] Clustering scores: {'NMI': 0.8907949449293677, 'ARI': 0.7548515641270315, 'AMI': 0.8736008795893361}\n",
      "[Model] ACC: 0.7538\n",
      "[Model] ACC sklearn: 0.0030\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 107\n",
      "[Representation] Clustering scores: {'NMI': 0.9488425030672353, 'ARI': 0.8460050151478561, 'AMI': 0.905436177345343}\n",
      "[Representation] ACC: 0.8559\n",
      "[Representation] ACC sklearn: 0.0000\n",
      "[Model] Clustering scores: {'NMI': 0.9193013615561652, 'ARI': 0.791217124445115, 'AMI': 0.8569905334518725}\n",
      "[Model] ACC: 0.7784\n",
      "[Model] ACC sklearn: 0.0000\n",
      "[2200]-----\n",
      "contrastive_local_loss:\t 0.02351\n",
      "contrastive_global_loss:\t 0.00696\n",
      "clustering_loss:\t 0.01481\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 116\n",
      "[Representation] Clustering scores: {'NMI': 0.9245356760561715, 'ARI': 0.8120173382237947, 'AMI': 0.9107594837751481}\n",
      "[Representation] ACC: 0.8213\n",
      "[Representation] ACC sklearn: 0.0078\n",
      "[Model] Clustering scores: {'NMI': 0.8859305620942366, 'ARI': 0.7578666426807266, 'AMI': 0.8686466086421886}\n",
      "[Model] ACC: 0.7515\n",
      "[Model] ACC sklearn: 0.0033\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 104\n",
      "[Representation] Clustering scores: {'NMI': 0.9492263713307852, 'ARI': 0.8474739789173461, 'AMI': 0.9065937201976207}\n",
      "[Representation] ACC: 0.8532\n",
      "[Representation] ACC sklearn: 0.0009\n",
      "[Model] Clustering scores: {'NMI': 0.9176179553235158, 'ARI': 0.7982073232132015, 'AMI': 0.8557412808805787}\n",
      "[Model] ACC: 0.7829\n",
      "[Model] ACC sklearn: 0.0018\n",
      "[Representation] Clustering scores: {'NMI': 0.9206559063268356, 'ARI': 0.800171285391907, 'AMI': 0.9065082673027673}\n",
      "[Representation] ACC: 0.8135\n",
      "[Representation] ACC sklearn: 0.0447\n",
      "[Model] Clustering scores: {'NMI': 0.8857889520888104, 'ARI': 0.7572279452389543, 'AMI': 0.8685883492544794}\n",
      "[Model] ACC: 0.7465\n",
      "[Model] ACC sklearn: 0.0019\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 102\n",
      "[Representation] Clustering scores: {'NMI': 0.9423432543684719, 'ARI': 0.8324302653539236, 'AMI': 0.8945048692668426}\n",
      "[Representation] ACC: 0.8414\n",
      "[Representation] ACC sklearn: 0.0018\n",
      "[Model] Clustering scores: {'NMI': 0.9149456725652356, 'ARI': 0.7923476153994752, 'AMI': 0.8513978556752417}\n",
      "[Model] ACC: 0.7730\n",
      "[Model] ACC sklearn: 0.0027\n",
      "[2600]-----\n",
      "contrastive_local_loss:\t 0.02426\n",
      "contrastive_global_loss:\t 0.00689\n",
      "clustering_loss:\t 0.01564\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 113\n",
      "[Representation] Clustering scores: {'NMI': 0.9197306628198569, 'ARI': 0.7865763334627882, 'AMI': 0.9058544671277299}\n",
      "[Representation] ACC: 0.8111\n",
      "[Representation] ACC sklearn: 0.0080\n",
      "[Model] Clustering scores: {'NMI': 0.8855968088484117, 'ARI': 0.7264719964777531, 'AMI': 0.8683431165274688}\n",
      "[Model] ACC: 0.7389\n",
      "[Model] ACC sklearn: 0.0016\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 99\n",
      "[Representation] Clustering scores: {'NMI': 0.9437955823327293, 'ARI': 0.8247657331896299, 'AMI': 0.8977087740792923}\n",
      "[Representation] ACC: 0.8351\n",
      "[Representation] ACC sklearn: 0.0342\n",
      "[Model] Clustering scores: {'NMI': 0.9149835557089251, 'ARI': 0.7543160772032337, 'AMI': 0.8515787948965411}\n",
      "[Model] ACC: 0.7649\n",
      "[Model] ACC sklearn: 0.0000\n",
      "[2800]-----\n",
      "contrastive_local_loss:\t 0.02378\n",
      "contrastive_global_loss:\t 0.00697\n",
      "clustering_loss:\t 0.01507\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 115\n",
      "[Representation] Clustering scores: {'NMI': 0.9139666295365605, 'ARI': 0.7581090465359479, 'AMI': 0.8994389148770578}\n",
      "[Representation] ACC: 0.7987\n",
      "[Representation] ACC sklearn: 0.0018\n",
      "[Model] Clustering scores: {'NMI': 0.8824154645203548, 'ARI': 0.6866931438621494, 'AMI': 0.8648569185423064}\n",
      "[Model] ACC: 0.7278\n",
      "[Model] ACC sklearn: 0.0022\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 98\n",
      "[Representation] Clustering scores: {'NMI': 0.9396417264148309, 'ARI': 0.8065697270309737, 'AMI': 0.8910154912900584}\n",
      "[Representation] ACC: 0.8270\n",
      "[Representation] ACC sklearn: 0.0000\n",
      "[Model] Clustering scores: {'NMI': 0.9091201776886623, 'ARI': 0.7258791851700521, 'AMI': 0.8413554304221236}\n",
      "[Model] ACC: 0.7523\n",
      "[Model] ACC sklearn: 0.0027\n"
     ]
    }
   ],
   "source": [
    "resPath, tensorboard = setup_path(args)\n",
    "args.resPath, args.tensorboard = resPath, tensorboard\n",
    "set_global_random_seed(args.seed)\n",
    "\n",
    "# Dataset loader\n",
    "train_loader = augment_loader(args)\n",
    "\n",
    "# torch.cuda.set_device(args.gpuid[0])\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# Initialize cluster centers\n",
    "# by performing k-means after getting embeddings from Sentence-BERT with mean-pooling(defualt)\n",
    "sbert = SentenceTransformer(MODEL_CLASS[args.bert])\n",
    "cluster_centers = get_kmeans_centers(sbert, train_loader, args.num_classes) \n",
    "\n",
    "\n",
    "# Model\n",
    "# 1. Transformer model \n",
    "# use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "# word_embedding_model = models.Transformer(MODEL_CLASS[args.bert])\n",
    "\n",
    "word_embedding_model = models.Transformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "#word_embedding_model = models.Transformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "dimension = word_embedding_model.get_word_embedding_dimension()\n",
    "# word_embedding_model = torch.nn.DataParallel(word_embedding_model)\n",
    "\n",
    "\n",
    "# 2. CNN model\n",
    "# cnn = models.CNN(in_word_embedding_dimension = word_embedding_model.get_word_embedding_dimension(), \n",
    "#                  use_cnn = args.use_cnn, out_channels = word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. Pooling \n",
    "# pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
    "#                                pooling_mode_mean_tokens=True,\n",
    "#                                pooling_mode_cls_token=False,\n",
    "#                                pooling_mode_max_tokens=False)\n",
    "pooling_model = models.Pooling(dimension,\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False, \n",
    "                               pooling_mode_weighted_tokens = False)\n",
    "\n",
    "# 4. Feature extractor \n",
    "#feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, cnn, pooling_model])\n",
    "feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, pooling_model], device = 'cuda')\n",
    "\n",
    "# 5. main model\n",
    "model = SCCLBert(feature_extractor, cluster_centers=cluster_centers, alpha = args.alpha, use_head = args.use_head)  \n",
    "\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':word_embedding_model.parameters(), 'lr': args.lr*6},\n",
    "#    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "    {'params':pooling_model.parameters()},\n",
    "#    {'params':model.head.parameters(), 'lr': args.lr*20},\n",
    "    {'params':model.cluster_centers, 'lr': args.lr*60}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params':word_embedding_model.parameters(), 'lr': args.lr},\n",
    "# #    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "#     {'params':pooling_model.parameters()},\n",
    "# #    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "#     {'params':model.cluster_centers, 'lr': args.lr*20}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "# Set up the trainer    \n",
    "learner = ClusterLearner(model, feature_extractor, optimizer, args.temperature, args.base_temperature,\n",
    "                         args.contrastive_local_scale, args.contrastive_global_scale, args.clustering_scale, use_head = args.use_head, use_normalize = args.use_normalize)\n",
    "# learner = torch.nn.DataParallel(learner)\n",
    "learner = learner.cuda()\n",
    "\n",
    "# split train - validation\n",
    "if(args.train_val_ratio != -1):\n",
    "    train_loader, val_loader = augment_loader_split(args)\n",
    "    training(train_loader, learner, args, val_loader = val_loader)\n",
    "# normal\n",
    "else:\n",
    "    training(train_loader, learner, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
