{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers_local import models, losses, SentenceTransformerSequential\n",
    "from models.Transformers import SCCLBert\n",
    "from learners.cluster import ClusterLearner\n",
    "from dataloader.dataloader import augment_loader, augment_loader_split\n",
    "from training import training\n",
    "from utils.kmeans import get_kmeans_centers\n",
    "from utils.logger import setup_path\n",
    "from utils.randomness import set_global_random_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n",
      "Tesla V100-SXM2-32GB-LS\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = {\n",
    "    \"distil\": 'distilbert-base-nli-stsb-mean-tokens', \n",
    "    \"robertabase\": 'roberta-base-nli-stsb-mean-tokens',\n",
    "    \"robertalarge\": 'roberta-large-nli-stsb-mean-tokens',\n",
    "    \"msmarco\": 'distilroberta-base-msmarco-v2',\n",
    "    \"xlm\": \"xlm-r-distilroberta-base-paraphrase-v1\",\n",
    "    \"bertlarge\": 'bert-large-nli-stsb-mean-tokens',\n",
    "    \"bertbase\": 'bert-base-nli-stsb-mean-tokens',\n",
    "    \"paraphrase\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-distil\": \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-Tiny\" : \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"stanford-sentiment-roberta\" : \"stanford-sentiment-treebank-roberta.2021-03-11\"\n",
    "}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0], help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "parser.add_argument('--seed', type=int, default=0, help=\"\")\n",
    "parser.add_argument('--print_freq', type=float, default=200, help=\"\")  \n",
    "parser.add_argument('--result_path', type=str, default='./results/')\n",
    "\n",
    "parser.add_argument('--bert', type=str, default='paraphrase', help=\"\")\n",
    "#parser.add_argument('--bert', type=str, default='distil', help=\"\")\n",
    "\n",
    "parser.add_argument('--bert_model', type=str, default='bert-base-uncased', help=\"\")\n",
    "parser.add_argument('--note', type=str, default='_search_snippets_distil_lre-4_JSD', help=\"\")\n",
    "\n",
    "# Dataset\n",
    "# stackoverflow/stackoverflow_true_text\n",
    "parser.add_argument('--dataset', type=str, default='search_snippets', help=\"\")\n",
    "#parser.add_argument('--dataset', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "parser.add_argument('--max_length', type=int, default=32)\n",
    "parser.add_argument('--train_val_ratio', type=float, default= [0.9, 0.1])\n",
    "\n",
    "# Data for train and test\n",
    "# ###### AgNews\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=4, help=\"\")\n",
    "# ####### SearchSnippets\n",
    "parser.add_argument('--data_path', type=str, default='./datasets/augmented/contextual_30_2col_distilbert/')\n",
    "# ## parser.add_argument('--dataname', type=str, default='train_search_snippets.csv', help=\"\")\n",
    "# ## parser.add_argument('--dataname_val', type=str, default='test_search_snippets.csv', help=\"\")\n",
    "# parser.add_argument('--dataname', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=8, help=\"\")\n",
    "# ###### StackOverFlow\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "# parser.add_argument('--dataname', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ###### Biomedical\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/biomedical/')\n",
    "# parser.add_argument('--dataname', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ######## Tweet\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=89, help=\"\")\n",
    "# ######## GoogleNewsTS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsT\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "parser.add_argument('--dataname', type=str, default='S', help=\"\")\n",
    "parser.add_argument('--dataname_val', type=str, default='S', help=\"\")\n",
    "parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "\n",
    "# Learning parameters\n",
    "parser.add_argument('--lr', type=float, default=1e-6, help=\"\") #learning rate\n",
    "parser.add_argument('--lr_scale', type=int, default=100, help=\"\")\n",
    "parser.add_argument('--max_iter', type=int, default=30000)\n",
    "parser.add_argument('--batch_size', type=int, default=256) #batch size\n",
    "\n",
    "# CNN Setting\n",
    "#parser.add_argument('--out_channels', type=int, default=768)\n",
    "#parser.add_argument('--use_cnn', type5yh=str, default='cnn_1')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_3')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_5')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_7')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_cat')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_avg')\n",
    "\n",
    "# Contrastive learning\n",
    "parser.add_argument('--use_head', type=bool, default=False)\n",
    "parser.add_argument('--use_normalize', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--weighted_local', type=bool, default= False, help=\"\")\n",
    "#parser.add_argument('--normalize_method', type=str, default='inverse_prob', help=\"\")\n",
    "parser.add_argument('--normalize_method', type=str, default='none', help=\"\")\n",
    "\n",
    "parser.add_argument('--contrastive_local_scale', type=float, default=0.002) #scale of contrastive loss\n",
    "parser.add_argument('--contrastive_global_scale', type=float, default=0.008) #scale of contrastive loss\n",
    "parser.add_argument('--temperature', type=float, default=0.5, help=\"temperature required by contrastive loss\")\n",
    "parser.add_argument('--base_temperature', type=float, default=0.1, help=\"temperature required by contrastive loss\")\n",
    "\n",
    "# Clustering\n",
    "parser.add_argument('--clustering_scale', type=float, default=0.02) #scale of clustering loss\n",
    "parser.add_argument('--use_perturbation', action='store_true', help=\"\")\n",
    "parser.add_argument('--alpha', type=float, default=1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args.use_gpu = args.gpuid[0] >= 0\n",
    "args.resPath = None\n",
    "args.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results path: ./results/SCCL.paraphrase.search_snippets.lr1e-06.lrscale100.tmp0.5.alpha1.seed0/\n",
      "all_embeddings:(11108, 768), true_labels:11108, pred_labels:11108\n",
      "true_labels tensor([19, 49, 13,  ..., 38, 15, 78])\n",
      "pred_labels tensor([10, 43, 96,  ..., 54, 86, 21], dtype=torch.int32)\n",
      "Iterations:19, Clustering ACC:0.648, centers:(152, 768)\n",
      "initial_cluster_centers =  torch.Size([152, 768])\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 6e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5.9999999999999995e-05\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample 0.9 9998\n",
      "val_sample 0.1 1110\n",
      "\n",
      "=30000/40=Iterations/Batches\n",
      "[0]-----\n",
      "contrastive_local_loss:\t 0.01379\n",
      "contrastive_global_loss:\t 0.01061\n",
      "clustering_loss:\t 0.00049\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.8779142551910439, 'ARI': 0.6189771468736401, 'AMI': 0.8517748508062367}\n",
      "[Representation] ACC: 0.6770\n",
      "[Representation] ACC sklearn: 0.0004\n",
      "[Model] Clustering scores: {'NMI': 0.8689544114887049, 'ARI': 0.5795887412430994, 'AMI': 0.8406301247098938}\n",
      "[Model] ACC: 0.6474\n",
      "[Model] ACC sklearn: 0.0117\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.8836993286063497, 'ARI': 0.548929832806624, 'AMI': 0.7531285160173455}\n",
      "[Representation] ACC: 0.6468\n",
      "[Representation] ACC sklearn: 0.0018\n",
      "[Model] Clustering scores: {'NMI': 0.8962768365662098, 'ARI': 0.589268030681512, 'AMI': 0.7802440666713153}\n",
      "[Model] ACC: 0.6856\n",
      "[Model] ACC sklearn: 0.0099\n",
      "[200]-----\n",
      "contrastive_local_loss:\t 0.00782\n",
      "contrastive_global_loss:\t 0.00589\n",
      "clustering_loss:\t 0.00044\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.8839080013126587, 'ARI': 0.6242674526177936, 'AMI': 0.8591744470539981}\n",
      "[Representation] ACC: 0.6894\n",
      "[Representation] ACC sklearn: 0.0002\n",
      "[Model] Clustering scores: {'NMI': 0.8693122372014231, 'ARI': 0.5802568332735651, 'AMI': 0.8411316365081701}\n",
      "[Model] ACC: 0.6537\n",
      "[Model] ACC sklearn: 0.0113\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.8990587972513548, 'ARI': 0.6042316457316244, 'AMI': 0.7873755785243493}\n",
      "[Representation] ACC: 0.6892\n",
      "[Representation] ACC sklearn: 0.0036\n",
      "[Model] Clustering scores: {'NMI': 0.8954416312439915, 'ARI': 0.585822703569544, 'AMI': 0.7790544948411838}\n",
      "[Model] ACC: 0.6865\n",
      "[Model] ACC sklearn: 0.0099\n",
      "[400]-----\n",
      "contrastive_local_loss:\t 0.00826\n",
      "contrastive_global_loss:\t 0.00591\n",
      "clustering_loss:\t 0.00085\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.8865097278184579, 'ARI': 0.658336823476244, 'AMI': 0.8627091447157464}\n",
      "[Representation] ACC: 0.7057\n",
      "[Representation] ACC sklearn: 0.0052\n",
      "[Model] Clustering scores: {'NMI': 0.8756201800237942, 'ARI': 0.5996710902802752, 'AMI': 0.8488677708684091}\n",
      "[Model] ACC: 0.6680\n",
      "[Model] ACC sklearn: 0.0108\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.9134907714073174, 'ARI': 0.6395581672860516, 'AMI': 0.8190802804726955}\n",
      "[Representation] ACC: 0.7180\n",
      "[Representation] ACC sklearn: 0.0054\n",
      "[Model] Clustering scores: {'NMI': 0.9042574794666646, 'ARI': 0.6123022499827524, 'AMI': 0.7975183384720669}\n",
      "[Model] ACC: 0.6991\n",
      "[Model] ACC sklearn: 0.0090\n",
      "[600]-----\n",
      "contrastive_local_loss:\t 0.00949\n",
      "contrastive_global_loss:\t 0.00603\n",
      "clustering_loss:\t 0.00193\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 152\n",
      "[Representation] Clustering scores: {'NMI': 0.8968265735306545, 'ARI': 0.7279202024939884, 'AMI': 0.8753844508666146}\n",
      "[Representation] ACC: 0.7677\n",
      "[Representation] ACC sklearn: 0.0114\n",
      "[Model] Clustering scores: {'NMI': 0.8773620099232995, 'ARI': 0.6175227244280186, 'AMI': 0.851172262925852}\n",
      "[Model] ACC: 0.6817\n",
      "[Model] ACC sklearn: 0.0113\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 151\n",
      "[Representation] Clustering scores: {'NMI': 0.9181258507425123, 'ARI': 0.6839150406167921, 'AMI': 0.8295440759249137}\n",
      "[Representation] ACC: 0.7495\n",
      "[Representation] ACC sklearn: 0.0009\n",
      "[Model] Clustering scores: {'NMI': 0.9101820889052373, 'ARI': 0.6479556907316507, 'AMI': 0.8115650392826524}\n",
      "[Model] ACC: 0.7180\n",
      "[Model] ACC sklearn: 0.0081\n",
      "[800]-----\n",
      "contrastive_local_loss:\t 0.01155\n",
      "contrastive_global_loss:\t 0.00623\n",
      "clustering_loss:\t 0.00375\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 151\n",
      "[Representation] Clustering scores: {'NMI': 0.8971304925475504, 'ARI': 0.766909614575233, 'AMI': 0.8762434606732563}\n",
      "[Representation] ACC: 0.7814\n",
      "[Representation] ACC sklearn: 0.0006\n",
      "[Model] Clustering scores: {'NMI': 0.8767280546495173, 'ARI': 0.6397851997122854, 'AMI': 0.8514130024302785}\n",
      "[Model] ACC: 0.7007\n",
      "[Model] ACC sklearn: 0.0131\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 146\n",
      "[Representation] Clustering scores: {'NMI': 0.9244934760635877, 'ARI': 0.7501216051981935, 'AMI': 0.8461945505129788}\n",
      "[Representation] ACC: 0.7766\n",
      "[Representation] ACC sklearn: 0.0045\n",
      "[Model] Clustering scores: {'NMI': 0.9064531605672891, 'ARI': 0.656430866709685, 'AMI': 0.8077565004327736}\n",
      "[Model] ACC: 0.7252\n",
      "[Model] ACC sklearn: 0.0099\n",
      "[1000]-----\n",
      "contrastive_local_loss:\t 0.01413\n",
      "contrastive_global_loss:\t 0.00643\n",
      "clustering_loss:\t 0.00607\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 147\n",
      "[Representation] Clustering scores: {'NMI': 0.9005927794767321, 'ARI': 0.7684755917803238, 'AMI': 0.8812820004052342}\n",
      "[Representation] ACC: 0.7942\n",
      "[Representation] ACC sklearn: 0.0015\n",
      "[Model] Clustering scores: {'NMI': 0.8693426976825724, 'ARI': 0.6388693506431083, 'AMI': 0.8442677577434221}\n",
      "[Model] ACC: 0.7052\n",
      "[Model] ACC sklearn: 0.0169\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 136\n",
      "[Representation] Clustering scores: {'NMI': 0.9320533479926527, 'ARI': 0.7656502416379021, 'AMI': 0.8651147665876289}\n",
      "[Representation] ACC: 0.8153\n",
      "[Representation] ACC sklearn: 0.0108\n",
      "[Model] Clustering scores: {'NMI': 0.9042724832151278, 'ARI': 0.6451973030062934, 'AMI': 0.8115558669076257}\n",
      "[Model] ACC: 0.7279\n",
      "[Model] ACC sklearn: 0.0099\n",
      "[1200]-----\n",
      "contrastive_local_loss:\t 0.01644\n",
      "contrastive_global_loss:\t 0.00656\n",
      "clustering_loss:\t 0.00822\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 142\n",
      "[Representation] Clustering scores: {'NMI': 0.895237903080109, 'ARI': 0.6990013184887027, 'AMI': 0.8754568611821651}\n",
      "[Representation] ACC: 0.7718\n",
      "[Representation] ACC sklearn: 0.0221\n",
      "[Model] Clustering scores: {'NMI': 0.8599706634299089, 'ARI': 0.6293959894189064, 'AMI': 0.8351740250747233}\n",
      "[Model] ACC: 0.6951\n",
      "[Model] ACC sklearn: 0.0224\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 133\n",
      "[Representation] Clustering scores: {'NMI': 0.9230838080192119, 'ARI': 0.6286664454185543, 'AMI': 0.8526984287306355}\n",
      "[Representation] ACC: 0.7658\n",
      "[Representation] ACC sklearn: 0.0018\n",
      "[Model] Clustering scores: {'NMI': 0.8920228382341929, 'ARI': 0.5917510563072579, 'AMI': 0.7937386662480422}\n",
      "[Model] ACC: 0.7099\n",
      "[Model] ACC sklearn: 0.0180\n",
      "[1400]-----\n",
      "contrastive_local_loss:\t 0.01827\n",
      "contrastive_global_loss:\t 0.00667\n",
      "clustering_loss:\t 0.00992\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 131\n",
      "[Representation] Clustering scores: {'NMI': 0.8994088138767488, 'ARI': 0.6949469321438188, 'AMI': 0.8808981688308377}\n",
      "[Representation] ACC: 0.7815\n",
      "[Representation] ACC sklearn: 0.0052\n",
      "[Model] Clustering scores: {'NMI': 0.8556272644959507, 'ARI': 0.6329978469042954, 'AMI': 0.8316057622620405}\n",
      "[Model] ACC: 0.6946\n",
      "[Model] ACC sklearn: 0.0229\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 124\n",
      "[Representation] Clustering scores: {'NMI': 0.921092876291792, 'ARI': 0.617940946034112, 'AMI': 0.8506511809602325}\n",
      "[Representation] ACC: 0.7613\n",
      "[Representation] ACC sklearn: 0.0216\n",
      "[Model] Clustering scores: {'NMI': 0.8877373795377302, 'ARI': 0.5962518761117891, 'AMI': 0.7913207730510132}\n",
      "[Model] ACC: 0.7072\n",
      "[Model] ACC sklearn: 0.0180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600]-----\n",
      "contrastive_local_loss:\t 0.02008\n",
      "contrastive_global_loss:\t 0.00687\n",
      "clustering_loss:\t 0.01150\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 126\n",
      "[Representation] Clustering scores: {'NMI': 0.8926589477896918, 'ARI': 0.6830899641379915, 'AMI': 0.8735911393363307}\n",
      "[Representation] ACC: 0.7706\n",
      "[Representation] ACC sklearn: 0.0521\n",
      "[Model] Clustering scores: {'NMI': 0.8477406450941565, 'ARI': 0.6338489057477635, 'AMI': 0.8234444273648279}\n",
      "[Model] ACC: 0.6870\n",
      "[Model] ACC sklearn: 0.0229\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 112\n",
      "[Representation] Clustering scores: {'NMI': 0.9188658267543058, 'ARI': 0.6143326694506388, 'AMI': 0.8477177578482396}\n",
      "[Representation] ACC: 0.7604\n",
      "[Representation] ACC sklearn: 0.0171\n",
      "[Model] Clustering scores: {'NMI': 0.8770615657309306, 'ARI': 0.587516049297366, 'AMI': 0.7750374811794265}\n",
      "[Model] ACC: 0.6937\n",
      "[Model] ACC sklearn: 0.0171\n",
      "[1800]-----\n",
      "contrastive_local_loss:\t 0.02092\n",
      "contrastive_global_loss:\t 0.00679\n",
      "clustering_loss:\t 0.01242\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 119\n",
      "[Representation] Clustering scores: {'NMI': 0.8878240658116828, 'ARI': 0.6781391124489439, 'AMI': 0.8679556007112345}\n",
      "[Representation] ACC: 0.7631\n",
      "[Representation] ACC sklearn: 0.0312\n",
      "[Model] Clustering scores: {'NMI': 0.8390684004261019, 'ARI': 0.6152399883578393, 'AMI': 0.814182663857256}\n",
      "[Model] ACC: 0.6665\n",
      "[Model] ACC sklearn: 0.0198\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 104\n",
      "[Representation] Clustering scores: {'NMI': 0.9145503728989325, 'ARI': 0.6049195516268794, 'AMI': 0.8407068581643787}\n",
      "[Representation] ACC: 0.7523\n",
      "[Representation] ACC sklearn: 0.0324\n",
      "[Model] Clustering scores: {'NMI': 0.8654724483710421, 'ARI': 0.5638928440918335, 'AMI': 0.7566718060899446}\n",
      "[Model] ACC: 0.6550\n",
      "[Model] ACC sklearn: 0.0117\n",
      "[2000]-----\n",
      "contrastive_local_loss:\t 0.02129\n",
      "contrastive_global_loss:\t 0.00722\n",
      "clustering_loss:\t 0.01225\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 116\n",
      "[Representation] Clustering scores: {'NMI': 0.8846845111700058, 'ARI': 0.6759860650746801, 'AMI': 0.8649126114626889}\n",
      "[Representation] ACC: 0.7502\n",
      "[Representation] ACC sklearn: 0.0215\n",
      "[Model] Clustering scores: {'NMI': 0.8335684629559729, 'ARI': 0.6236824811105268, 'AMI': 0.8084604039623268}\n",
      "[Model] ACC: 0.6665\n",
      "[Model] ACC sklearn: 0.0124\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 102\n",
      "[Representation] Clustering scores: {'NMI': 0.9090494493997532, 'ARI': 0.5984131945501318, 'AMI': 0.8329276043726356}\n",
      "[Representation] ACC: 0.7405\n",
      "[Representation] ACC sklearn: 0.0000\n",
      "[Model] Clustering scores: {'NMI': 0.8615291381780827, 'ARI': 0.57159632225731, 'AMI': 0.7515888352645904}\n",
      "[Model] ACC: 0.6541\n",
      "[Model] ACC sklearn: 0.0072\n",
      "[2200]-----\n",
      "contrastive_local_loss:\t 0.02292\n",
      "contrastive_global_loss:\t 0.00720\n",
      "clustering_loss:\t 0.01393\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 108\n",
      "[Representation] Clustering scores: {'NMI': 0.8724271040959692, 'ARI': 0.6512794614201373, 'AMI': 0.851742852666794}\n",
      "[Representation] ACC: 0.7300\n",
      "[Representation] ACC sklearn: 0.0377\n",
      "[Model] Clustering scores: {'NMI': 0.8301826228459738, 'ARI': 0.5940868187269684, 'AMI': 0.8053142048621538}\n",
      "[Model] ACC: 0.6495\n",
      "[Model] ACC sklearn: 0.0101\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 98\n",
      "[Representation] Clustering scores: {'NMI': 0.8998623653984726, 'ARI': 0.5821074820924603, 'AMI': 0.8189967318571091}\n",
      "[Representation] ACC: 0.7135\n",
      "[Representation] ACC sklearn: 0.0342\n",
      "[Model] Clustering scores: {'NMI': 0.8592726752657213, 'ARI': 0.5278616852570917, 'AMI': 0.7520421978220472}\n",
      "[Model] ACC: 0.6351\n",
      "[Model] ACC sklearn: 0.0054\n",
      "[2400]-----\n",
      "contrastive_local_loss:\t 0.02352\n",
      "contrastive_global_loss:\t 0.00717\n",
      "clustering_loss:\t 0.01456\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 106\n",
      "[Representation] Clustering scores: {'NMI': 0.866746988194995, 'ARI': 0.642480729934934, 'AMI': 0.8460375753480963}\n",
      "[Representation] ACC: 0.7196\n",
      "[Representation] ACC sklearn: 0.0013\n",
      "[Model] Clustering scores: {'NMI': 0.830023842982778, 'ARI': 0.5815771125850426, 'AMI': 0.8068020760510475}\n",
      "[Model] ACC: 0.6579\n",
      "[Model] ACC sklearn: 0.0078\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 93\n",
      "[Representation] Clustering scores: {'NMI': 0.898483488794491, 'ARI': 0.5798304101208693, 'AMI': 0.8169988282876983}\n",
      "[Representation] ACC: 0.7045\n",
      "[Representation] ACC sklearn: 0.0027\n",
      "[Model] Clustering scores: {'NMI': 0.8618413321460346, 'ARI': 0.5126268220092921, 'AMI': 0.7604782954620231}\n",
      "[Model] ACC: 0.6333\n",
      "[Model] ACC sklearn: 0.0063\n",
      "[Representation] Clustering scores: {'NMI': 0.8841373705810561, 'ARI': 0.5546423477142993, 'AMI': 0.7941617870287263}\n",
      "[Representation] ACC: 0.6766\n",
      "[Representation] ACC sklearn: 0.0081\n",
      "[Model] Clustering scores: {'NMI': 0.853236218628714, 'ARI': 0.48661311923167117, 'AMI': 0.7464776520824652}\n",
      "[Model] ACC: 0.6189\n",
      "[Model] ACC sklearn: 0.0063\n",
      "[2800]-----\n",
      "contrastive_local_loss:\t 0.02460\n",
      "contrastive_global_loss:\t 0.00727\n",
      "clustering_loss:\t 0.01552\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 102\n",
      "[Representation] Clustering scores: {'NMI': 0.8488597400914014, 'ARI': 0.5985528023832034, 'AMI': 0.8289776490554405}\n",
      "[Representation] ACC: 0.6839\n",
      "[Representation] ACC sklearn: 0.0101\n",
      "[Model] Clustering scores: {'NMI': 0.8274075343235939, 'ARI': 0.5509699574986884, 'AMI': 0.8039500361198856}\n",
      "[Model] ACC: 0.6409\n",
      "[Model] ACC sklearn: 0.0072\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 92\n",
      "[Representation] Clustering scores: {'NMI': 0.8765707212376869, 'ARI': 0.5397601234701079, 'AMI': 0.7813907816606473}\n",
      "[Representation] ACC: 0.6577\n",
      "[Representation] ACC sklearn: 0.0081\n",
      "[Model] Clustering scores: {'NMI': 0.8560229758471104, 'ARI': 0.4829162173869775, 'AMI': 0.7512653557816708}\n",
      "[Model] ACC: 0.6270\n",
      "[Model] ACC sklearn: 0.0063\n",
      "[3000]-----\n",
      "contrastive_local_loss:\t 0.02452\n",
      "contrastive_global_loss:\t 0.00745\n",
      "clustering_loss:\t 0.01523\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 95\n",
      "[Representation] Clustering scores: {'NMI': 0.847587751153288, 'ARI': 0.5948040908958008, 'AMI': 0.827920216974867}\n",
      "[Representation] ACC: 0.6819\n",
      "[Representation] ACC sklearn: 0.0087\n",
      "[Model] Clustering scores: {'NMI': 0.8300224790825493, 'ARI': 0.5790238687924639, 'AMI': 0.8078635775259264}\n",
      "[Model] ACC: 0.6526\n",
      "[Model] ACC sklearn: 0.0065\n",
      "------------- Evaluate Validation Set -------------\n",
      "------------- 5 batches -------------\n",
      "all_pred 88\n",
      "[Representation] Clustering scores: {'NMI': 0.8707111752631975, 'ARI': 0.5536242666960679, 'AMI': 0.7689706478539657}\n",
      "[Representation] ACC: 0.6541\n",
      "[Representation] ACC sklearn: 0.0018\n",
      "[Model] Clustering scores: {'NMI': 0.8548261471448603, 'ARI': 0.5142444688600178, 'AMI': 0.7531894834683107}\n",
      "[Model] ACC: 0.6324\n",
      "[Model] ACC sklearn: 0.0045\n",
      "[3200]-----\n",
      "contrastive_local_loss:\t 0.02461\n",
      "contrastive_global_loss:\t 0.00747\n",
      "clustering_loss:\t 0.01529\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 40 batches -------------\n",
      "all_pred 96\n"
     ]
    }
   ],
   "source": [
    "resPath, tensorboard = setup_path(args)\n",
    "args.resPath, args.tensorboard = resPath, tensorboard\n",
    "set_global_random_seed(args.seed)\n",
    "\n",
    "# Dataset loader\n",
    "train_loader = augment_loader(args)\n",
    "\n",
    "# torch.cuda.set_device(args.gpuid[0])\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# Initialize cluster centers\n",
    "# by performing k-means after getting embeddings from Sentence-BERT with mean-pooling(defualt)\n",
    "sbert = SentenceTransformer(MODEL_CLASS[args.bert])\n",
    "cluster_centers = get_kmeans_centers(sbert, train_loader, args.num_classes) \n",
    "\n",
    "\n",
    "# Model\n",
    "# 1. Transformer model \n",
    "# use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "# word_embedding_model = models.Transformer(MODEL_CLASS[args.bert])\n",
    "\n",
    "word_embedding_model = models.Transformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "#word_embedding_model = models.Transformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "dimension = word_embedding_model.get_word_embedding_dimension()\n",
    "# word_embedding_model = torch.nn.DataParallel(word_embedding_model)\n",
    "\n",
    "\n",
    "# 2. CNN model\n",
    "# cnn = models.CNN(in_word_embedding_dimension = word_embedding_model.get_word_embedding_dimension(), \n",
    "#                  use_cnn = args.use_cnn, out_channels = word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. Pooling \n",
    "# pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
    "#                                pooling_mode_mean_tokens=True,\n",
    "#                                pooling_mode_cls_token=False,\n",
    "#                                pooling_mode_max_tokens=False)\n",
    "pooling_model = models.Pooling(dimension,\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False, \n",
    "                               pooling_mode_weighted_tokens = False)\n",
    "\n",
    "# 4. Feature extractor \n",
    "#feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, cnn, pooling_model])\n",
    "feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, pooling_model], device = 'cuda')\n",
    "\n",
    "# 5. main model\n",
    "model = SCCLBert(feature_extractor, cluster_centers=cluster_centers, alpha = args.alpha, use_head = args.use_head)  \n",
    "\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':word_embedding_model.parameters(), 'lr': args.lr*6},\n",
    "#    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "    {'params':pooling_model.parameters()},\n",
    "#    {'params':model.head.parameters(), 'lr': args.lr*6},\n",
    "    {'params':model.cluster_centers, 'lr': args.lr*60}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params':word_embedding_model.parameters(), 'lr': args.lr},\n",
    "# #    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "#     {'params':pooling_model.parameters()},\n",
    "# #    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "#     {'params':model.cluster_centers, 'lr': args.lr*20}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "# Set up the trainer    \n",
    "learner = ClusterLearner(model, feature_extractor, optimizer, args.temperature, args.base_temperature,\n",
    "                         args.contrastive_local_scale, args.contrastive_global_scale, args.clustering_scale, use_head = args.use_head, use_normalize = args.use_normalize)\n",
    "# learner = torch.nn.DataParallel(learner)\n",
    "learner = learner.cuda()\n",
    "# split train - validation\n",
    "if(args.train_val_ratio != -1):\n",
    "    train_loader, val_loader = augment_loader_split(args)\n",
    "    training(train_loader, learner, args, val_loader = val_loader)\n",
    "# normal\n",
    "else:\n",
    "    training(train_loader, learner, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
